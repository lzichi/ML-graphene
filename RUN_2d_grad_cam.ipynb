{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m8CvxOEKbObh",
        "tzwWrSQZvgda",
        "BpG3dlCSeKJ0"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lzichi/Thin-Materials-ML/blob/main/RUN_2d_grad_cam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSpQsdddK2YF",
        "outputId": "f6d0067b-0afb-47f5-bf4c-8721d947e7c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0BGQEGpeybV"
      },
      "source": [
        "# CAM codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyEWGSMmLALu"
      },
      "source": [
        "import torch\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "from torchvision import models, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLh3A7BDKbiY"
      },
      "source": [
        "class FeatureExtractor():\n",
        "    \"\"\" Class for extracting activations and\n",
        "    registering gradients from targetted intermediate layers \"\"\"\n",
        "\n",
        "    def __init__(self, model, target_layers):\n",
        "        self.model = model\n",
        "        self.target_layers = target_layers\n",
        "        self.gradients = []\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients.append(grad)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        outputs = []\n",
        "        self.gradients = []\n",
        "        for name, module in self.model._modules.items():\n",
        "            x = module(x)\n",
        "            if name in self.target_layers:\n",
        "                x.register_hook(self.save_gradient)\n",
        "                outputs += [x]\n",
        "        return outputs, x\n",
        "\n",
        "class ModelOutputs():\n",
        "    \"\"\" Class for making a forward pass, and getting:\n",
        "    1. The network output.\n",
        "    2. Activations from intermeddiate targetted layers.\n",
        "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
        "\n",
        "    def __init__(self, model, feature_module, target_layers):\n",
        "        self.model = model\n",
        "        self.feature_module = feature_module\n",
        "        self.feature_extractor = FeatureExtractor(self.feature_module, target_layers)\n",
        "\n",
        "    def get_gradients(self):\n",
        "        return self.feature_extractor.gradients\n",
        "\n",
        "    def __call__(self, x):\n",
        "        target_activations = []\n",
        "        for name, module in self.model._modules.items():\n",
        "            if module == self.feature_module:\n",
        "                target_activations, x = self.feature_extractor(x)\n",
        "            elif \"avgpool\" in name.lower():\n",
        "                x = module(x)\n",
        "                x = x.view(x.size(0),-1)\n",
        "            else:\n",
        "                x = module(x)\n",
        "\n",
        "        return target_activations, x\n",
        "\n",
        "def preprocess_image(img):\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "    preprocessing = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "    return preprocessing(img.copy()).unsqueeze(0)\n",
        "\n",
        "def show_cam_on_image(img, mask):\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    cam = cam / np.max(cam)\n",
        "    return np.uint8(255 * cam)\n",
        "\n",
        "class GradCam:\n",
        "    def __init__(self, model, feature_module, target_layer_names, use_cuda):\n",
        "        self.model = model\n",
        "        self.feature_module = feature_module\n",
        "        self.model.eval()\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "\n",
        "        self.extractor = ModelOutputs(self.model, self.feature_module, target_layer_names)\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        return self.model(input_img)\n",
        "\n",
        "    def __call__(self, input_img, target_category=None):\n",
        "        if self.cuda:\n",
        "            input_img = input_img.cuda()\n",
        "\n",
        "        features, output = self.extractor(input_img)\n",
        "\n",
        "        if target_category == None:\n",
        "            target_category = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "        one_hot[0][target_category] = 1\n",
        "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "        if self.cuda:\n",
        "            one_hot = one_hot.cuda()\n",
        "        \n",
        "        one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "        self.feature_module.zero_grad()\n",
        "        self.model.zero_grad()\n",
        "        one_hot.backward(retain_graph=True)\n",
        "\n",
        "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
        "\n",
        "        target = features[-1]\n",
        "        target = target.cpu().data.numpy()[0, :]\n",
        "\n",
        "        weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
        "        cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
        "\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * target[i, :, :]\n",
        "\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cv2.resize(cam, input_img.shape[2:])\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "\n",
        "class GuidedBackpropReLU(Function):\n",
        "    @staticmethod\n",
        "    def forward(self, input_img):\n",
        "        positive_mask = (input_img > 0).type_as(input_img)\n",
        "        output = torch.addcmul(torch.zeros(input_img.size()).type_as(input_img), input_img, positive_mask)\n",
        "        self.save_for_backward(input_img, output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        input_img, output = self.saved_tensors\n",
        "        grad_input = None\n",
        "\n",
        "        positive_mask_1 = (input_img > 0).type_as(grad_output)\n",
        "        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
        "        grad_input = torch.addcmul(torch.zeros(input_img.size()).type_as(input_img),\n",
        "                                   torch.addcmul(torch.zeros(input_img.size()).type_as(input_img), grad_output,\n",
        "                                                 positive_mask_1), positive_mask_2)\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class GuidedBackpropReLUModel:\n",
        "    def __init__(self, model, use_cuda):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "\n",
        "        def recursive_relu_apply(module_top):\n",
        "            for idx, module in module_top._modules.items():\n",
        "                recursive_relu_apply(module)\n",
        "                if module.__class__.__name__ == 'ReLU':\n",
        "                    module_top._modules[idx] = GuidedBackpropReLU.apply\n",
        "\n",
        "        # replace ReLU with GuidedBackpropReLU\n",
        "        recursive_relu_apply(self.model)\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        return self.model(input_img)\n",
        "\n",
        "    def __call__(self, input_img, target_category=None):\n",
        "        if self.cuda:\n",
        "            input_img = input_img.cuda()\n",
        "\n",
        "        input_img = input_img.requires_grad_(True)\n",
        "\n",
        "        output = self.forward(input_img)\n",
        "\n",
        "        if target_category == None:\n",
        "            target_category = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "        one_hot[0][target_category] = 1\n",
        "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "        if self.cuda:\n",
        "            one_hot = one_hot.cuda()\n",
        "\n",
        "        one_hot = torch.sum(one_hot * output)\n",
        "        one_hot.backward(retain_graph=True)\n",
        "\n",
        "        output = input_img.grad.cpu().data.numpy()\n",
        "        output = output[0, :, :, :]\n",
        "\n",
        "        return output\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--use-cuda', action='store_true', default=False,\n",
        "                        help='Use NVIDIA GPU acceleration')\n",
        "    parser.add_argument('--image-path', type=str, default='./examples/both.png',\n",
        "                        help='Input image path')\n",
        "    args = parser.parse_args()\n",
        "    args.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
        "    if args.use_cuda:\n",
        "        print(\"Using GPU for acceleration\")\n",
        "    else:\n",
        "        print(\"Using CPU for computation\")\n",
        "\n",
        "    return args\n",
        "\n",
        "# def deprocess_image(img):\n",
        "#     \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n",
        "#     img = img - np.mean(img)\n",
        "#     img = img / (np.std(img) + 1e-5)\n",
        "#     img = img * 0.1\n",
        "#     img = img + 0.5\n",
        "#     img = np.clip(img, 0, 1)\n",
        "#     return np.uint8(img*255)\n",
        "\n",
        "def deprocess_image(img):\n",
        "    return np.uint8(img*255)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJrl-DKle2WW"
      },
      "source": [
        "# 2d Codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB7kU3EQLIQF"
      },
      "source": [
        "import os, argparse, time, random\n",
        "from functools import partial\n",
        "from shutil import copyfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpqsanODM82L"
      },
      "source": [
        "import os\n",
        "\n",
        "def makedirs(*dirnames):\n",
        "    for dirname in dirnames:\n",
        "        if not os.path.exists(dirname):\n",
        "            os.makedirs(dirname)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "random.seed(41)\n",
        "np.random.seed(41)\n",
        "torch.manual_seed(41)\n",
        "torch.cuda.manual_seed_all(41)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wWTteiBMmAP"
      },
      "source": [
        "class FlakeDataset(Dataset):\n",
        "    def __init__(self, df, raw_only, transform=None, material=None):\n",
        "        paths, labels, materials, dat_type = [], [], [], []\n",
        "        for idx, path in enumerate(df['paths']):\n",
        "            file = path.split('/')[-1].split('.')[0].split('-')[-1]\n",
        "            if '_crop' in file:\n",
        "                file = file.split('_crop')[0]\n",
        "            \n",
        "            if material:\n",
        "                _is_target = [file in mat for mat in material]\n",
        "                if max(_is_target) is False:\n",
        "                    continue\n",
        "                # for mat in material:\n",
        "                #     if file in mat: \n",
        "                #         break\n",
        "                    # continue\n",
        "            # else:\n",
        "            if 'aug_' in path:\n",
        "                dat_type_i = 'augment'\n",
        "                if not raw_only:\n",
        "                    dat_type.append(dat_type_i)\n",
        "                    paths.append(path)\n",
        "                    labels.append(df.labels[idx])\n",
        "                    materials.append(file)\n",
        "            else:\n",
        "                dat_type_i = 'raw'\n",
        "                dat_type.append(dat_type_i)\n",
        "                paths.append(path)\n",
        "                labels.append(df.labels[idx])\n",
        "                materials.append(file)\n",
        "            \n",
        "            # print(valid)\n",
        "            # raise NotADirectoryError()\n",
        "        self.path = paths\n",
        "        self.labels = torch.tensor(labels).float()\n",
        "        self.materials = np.array(materials)\n",
        "        self.dat_type = np.array(dat_type)\n",
        "        \n",
        "        if not transform:\n",
        "            transform = transforms.Compose([transforms.ToTensor()])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        if torch.is_tensor(i):\n",
        "            i = i.tolist()\n",
        "            \n",
        "        img = Image.open(self.path[i]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = self.labels[[i]]\n",
        "        material = self.materials[i]\n",
        "        dat_type = self.dat_type[i]\n",
        "        return img.float(), label, material, dat_type\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEgJoPofM0cf"
      },
      "source": [
        "result_path = '/content/drive/Shared drives/2d/results/to_compare'\n",
        "\n",
        "makedirs(result_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAU_3zKpbNqS"
      },
      "source": [
        "def extract_filename(path):\n",
        "    path = path.split('/')[-1]\n",
        "    path = path.split('.')[0]\n",
        "    return path\n",
        "\n",
        "\n",
        "def train_test_split(data, train_portion=0.75, seed=1):\n",
        "    np.random.seed(seed)\n",
        "    assert 'names' in data.columns, f'`names` column is not found in df.'\n",
        "\n",
        "    full_data = data.copy()\n",
        "    full_data['original_name'] = full_data.names.apply(lambda x: '_'.join(x.split('_')[int('aug' in x or 'raw' in x) + int('aug' in x):]))\n",
        "    original_imgs = np.unique(full_data['original_name'])\n",
        "    assert original_imgs.shape[0] == 332\n",
        "    train_img = set(np.random.choice(original_imgs, \n",
        "                                 int(train_portion * len(original_imgs)),\n",
        "                                 False))\n",
        "    train_idx = full_data.original_name.apply(lambda x: x in train_img)\n",
        "    train_data = full_data[train_idx].drop('original_name', 1).reset_index()\n",
        "    test_data = full_data[~train_idx].drop('original_name', 1).reset_index()\n",
        "    \n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "def load_train_test(data):\n",
        "\n",
        "    train, test = train_test_split(data)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size=(224, 224)),\n",
        "        transforms.ToTensor()])\n",
        "\n",
        "    materials = ['MoSe2_on_Si', 'MoSe2_on_si_PDMS', 'MoSe2_on_PDMS']\n",
        "\n",
        "    trainset = FlakeDataset(train, raw_only=False,\n",
        "                        transform=transform, \n",
        "                        material=materials)\n",
        "\n",
        "    testset = FlakeDataset(test, raw_only=False,\n",
        "                        transform=transform, \n",
        "                        material=materials)\n",
        "\n",
        "    bsz = 4\n",
        "    train_loader = DataLoader(trainset, batch_size=bsz, shuffle=True, pin_memory=True)\n",
        "    test_loader = DataLoader(testset, batch_size=bsz, shuffle=True, pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def load_model(path):\n",
        "    net = models.resnet18(pretrained=False)\n",
        "    fc_features = net.fc.in_features\n",
        "    net.fc = nn.Linear(fc_features, 1)\n",
        "    net = net.to(device)\n",
        "    net.load_state_dict(\n",
        "        torch.load(path, map_location=device))\n",
        "    \n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Only correct predictions"
      ],
      "metadata": {
        "id": "jISpFiLQ2AZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_correct_prediction(net, input, label, material, dat_type):\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        input = input.to(device).unsqueeze(0)\n",
        "        label = label.to(device)\n",
        "        output = net(input)\n",
        "        pred = (output > 0).float()\n",
        "\n",
        "    net.train()\n",
        "    return pred == label"
      ],
      "metadata": {
        "id": "8JHUcjRe20j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in os.listdir(result_path):\n",
        "\n",
        "    # saved results\n",
        "    \n",
        "    # if file in ['quantized10']:\n",
        "    if file in ['resnet18.torch', 'quantized20', 'quantized5']:\n",
        "        continue\n",
        "\n",
        "    else:\n",
        "\n",
        "        if file == 'resnet18.torch':\n",
        "            \n",
        "            # original\n",
        "            \n",
        "            k = str(-1)\n",
        "            data = pd.read_pickle(os.path.join('/content/drive/Shared drives/2d/data', \n",
        "                                               'pad_augment_data_final.pkl'))\n",
        "            data['paths'] = data['paths'].apply(\n",
        "                lambda x: '/content/drive/Shared drives/2d/data/pad_augment_data_final/' + \n",
        "                x.split('/')[-1])\n",
        "            \n",
        "            net_path = os.path.join(result_path, file)\n",
        "            cam_fold = os.path.join(result_path, 'cam_correct', 'original')\n",
        "        else:\n",
        "            \n",
        "            # quantized\n",
        "            \n",
        "            k = file.replace('quantized', '')\n",
        "            data = pd.read_pickle(os.path.join('/content/drive/Shared drives/2d/data', \n",
        "                                               f'quantized{k}_pad_augment_data.pkl'))\n",
        "            data['paths'] = data['paths'].apply(\n",
        "                lambda x: f'/content/drive/Shared drives/2d/data/quantized{k}_pad_augment_data/' + \n",
        "                x.split('/')[-1])\n",
        "            \n",
        "            net_path = os.path.join(result_path, file, 'resnet18.torch')\n",
        "            cam_fold = os.path.join(result_path, 'cam_correct', file)\n",
        "\n",
        "        net = load_model(net_path)\n",
        "        train, test = load_train_test(data)\n",
        "        trainset = train.dataset\n",
        "        testset = test.dataset\n",
        "\n",
        "        cam_train = os.path.join(cam_fold, 'train')\n",
        "        cam_test = os.path.join(cam_fold, 'test')\n",
        "        makedirs(cam_fold, cam_train, cam_test)\n",
        "\n",
        "        target_category = 0\n",
        "        for i in range(len(trainset)):\n",
        "            is_correct = is_correct_prediction(net, *trainset[i])\n",
        "            if is_correct:\n",
        "                file_name = extract_filename(trainset.path[i])\n",
        "                img = trainset[i][0]\n",
        "                \n",
        "                input_img = img.unsqueeze(0)\n",
        "                img = img.permute((1, 2, 0))\n",
        "                net.zero_grad()\n",
        "                grad_cam = GradCam(model=net, feature_module=net.layer4, \\\n",
        "                                target_layer_names=[\"1\"], \n",
        "                                use_cuda=(device==torch.device('cuda')))\n",
        "            \n",
        "                grayscale_cam = grad_cam(input_img, target_category)\n",
        "                grayscale_cam = cv2.resize(grayscale_cam, (img.shape[1], img.shape[0]))\n",
        "                cam = show_cam_on_image(img, grayscale_cam)\n",
        "\n",
        "                cv2.imwrite(os.path.join(cam_train, 'cam_' + file_name + '.jpg'), cam)\n",
        "            \n",
        "        print(f'finish trainset: {cam_fold}')\n",
        "\n",
        "        for i in range(len(testset)):\n",
        "            is_correct = is_correct_prediction(net, *testset[i])\n",
        "            if is_correct:\n",
        "                file_name = extract_filename(testset.path[i])\n",
        "                img = testset[i][0]\n",
        "                \n",
        "                input_img = img.unsqueeze(0)\n",
        "                img = img.permute((1, 2, 0))\n",
        "                net.zero_grad()\n",
        "                grad_cam = GradCam(model=net, feature_module=net.layer4, \\\n",
        "                                target_layer_names=[\"1\"], \n",
        "                                use_cuda=(device==torch.device('cuda')))\n",
        "            \n",
        "                grayscale_cam = grad_cam(input_img, target_category)\n",
        "                grayscale_cam = cv2.resize(grayscale_cam, (img.shape[1], img.shape[0]))\n",
        "                cam = show_cam_on_image(img, grayscale_cam)\n",
        "\n",
        "                cv2.imwrite(os.path.join(cam_test, 'cam_' + file_name + '.jpg'), cam)\n",
        "        print(f'finish testset: {cam_fold}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEScY3AO1-z7",
        "outputId": "2e89185d-f79b-4464-f869-0ba333b7ee39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish trainset: /content/drive/Shared drives/2d/results/to_compare/cam_correct/original\n",
            "finish testset: /content/drive/Shared drives/2d/results/to_compare/cam_correct/original\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish trainset: /content/drive/Shared drives/2d/results/to_compare/cam_correct/quantized5\n",
            "finish testset: /content/drive/Shared drives/2d/results/to_compare/cam_correct/quantized5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish trainset: /content/drive/Shared drives/2d/results/to_compare/cam_correct/quantized10\n",
            "finish testset: /content/drive/Shared drives/2d/results/to_compare/cam_correct/quantized10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8CvxOEKbObh"
      },
      "source": [
        "# New result: 07/13"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH0zdXItcO-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce30ae0b-5927-4f1c-e06d-a36f21d3af94"
      },
      "source": [
        "for file in os.listdir(result_path):\n",
        "    # saved results\n",
        "    # if file in ['resnet18.torch', 'quantized5', 'quantized10']:\n",
        "    if file != 'quantized20':\n",
        "        continue\n",
        "\n",
        "    else:\n",
        "        if file == 'resnet18.torch':\n",
        "            # original\n",
        "            k = str(-1)\n",
        "            data = pd.read_pickle(os.path.join('/content/drive/Shared drives/2d/data', \n",
        "                                               'pad_augment_data_final.pkl'))\n",
        "            data['paths'] = data['paths'].apply(\n",
        "                lambda x: '/content/drive/Shared drives/2d/data/pad_augment_data_final/' + \n",
        "                x.split('/')[-1])\n",
        "            \n",
        "            net_path = os.path.join(result_path, file)\n",
        "            cam_fold = os.path.join(result_path, 'cam', 'original')\n",
        "        else:\n",
        "            # quantized\n",
        "            k = file.replace('quantized', '')\n",
        "            data = pd.read_pickle(os.path.join('/content/drive/Shared drives/2d/data', \n",
        "                                               f'quantized{k}_pad_augment_data.pkl'))\n",
        "            data['paths'] = data['paths'].apply(\n",
        "                lambda x: f'/content/drive/Shared drives/2d/data/quantized{k}_pad_augment_data/' + \n",
        "                x.split('/')[-1])\n",
        "            \n",
        "            net_path = os.path.join(result_path, file, 'resnet18.torch')\n",
        "            cam_fold = os.path.join(result_path, 'cam', file)\n",
        "\n",
        "        net = load_model(net_path)\n",
        "        train, test = load_train_test(data)\n",
        "        trainset = train.dataset\n",
        "        testset = test.dataset\n",
        "\n",
        "        cam_train = os.path.join(cam_fold, 'train')\n",
        "        cam_test = os.path.join(cam_fold, 'test')\n",
        "        makedirs(cam_fold, cam_train, cam_test)\n",
        "\n",
        "        target_category = 0\n",
        "        for i in range(len(trainset)):\n",
        "            file_name = extract_filename(trainset.path[i])\n",
        "            img = trainset[i][0]\n",
        "            \n",
        "            input_img = img.unsqueeze(0)\n",
        "            img = img.permute((1, 2, 0))\n",
        "\n",
        "            net.zero_grad()\n",
        "            # net = load_model(net_path)\n",
        "\n",
        "            grad_cam = GradCam(model=net, feature_module=net.layer4, \\\n",
        "                            target_layer_names=[\"1\"], \n",
        "                            use_cuda=(device==torch.device('cuda')))\n",
        "        \n",
        "            grayscale_cam = grad_cam(input_img, target_category)\n",
        "            grayscale_cam = cv2.resize(grayscale_cam, (img.shape[1], img.shape[0]))\n",
        "            cam = show_cam_on_image(img, grayscale_cam)\n",
        "\n",
        "            cv2.imwrite(os.path.join(cam_train, 'cam_' + file_name + '.jpg'), cam)\n",
        "        \n",
        "        print(f'finish trainset: {cam_fold}')\n",
        "\n",
        "        for i in range(len(testset)):\n",
        "            file_name = extract_filename(testset.path[i])\n",
        "            img = testset[i][0]\n",
        "            \n",
        "            input_img = img.unsqueeze(0)\n",
        "            img = img.permute((1, 2, 0))\n",
        "            net.zero_grad()\n",
        "            grad_cam = GradCam(model=net, feature_module=net.layer4, \\\n",
        "                            target_layer_names=[\"1\"], \n",
        "                            use_cuda=(device==torch.device('cuda')))\n",
        "        \n",
        "            grayscale_cam = grad_cam(input_img, target_category)\n",
        "            grayscale_cam = cv2.resize(grayscale_cam, (img.shape[1], img.shape[0]))\n",
        "            cam = show_cam_on_image(img, grayscale_cam)\n",
        "\n",
        "            cv2.imwrite(os.path.join(cam_test, 'cam_' + file_name + '.jpg'), cam)\n",
        "        print(f'finish testset: {cam_fold}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finish trainset: /content/drive/Shared drives/2d/results/to_compare/cam/quantized20\n",
            "finish testset: /content/drive/Shared drives/2d/results/to_compare/cam/quantized20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKwgb1b2vbqR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeilIvlAvbwZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j421JaodkIq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ucr4usvb7R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9ZJ1qJ0vcGV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "popHkM49vcOm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzwWrSQZvgda"
      },
      "source": [
        "# Old results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHj43osffg26"
      },
      "source": [
        "def torch_dict_path(exp_path, train_material):\n",
        "    if os.path.exists(os.path.join(exp_path,\n",
        "                                    '&'.join(train_material) + \n",
        "                                    '.torch')):\n",
        "        path = os.path.join(exp_path,\n",
        "                                    '&'.join(train_material) + \n",
        "                                    '.torch')\n",
        "    else:\n",
        "        path = os.path.join(exp_path,\n",
        "                                    '&'.join(train_material[::-1]) + \n",
        "                                    '.torch')\n",
        "    return path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdJkZouOMuWw",
        "outputId": "0a8b7b67-a3a2-4674-f287-24dff9ec17f7"
      },
      "source": [
        "for test_mat in materials:\n",
        "    train_material = [mat for mat in materials if mat != test_mat]\n",
        "    trainset = FlakeDataset(data, raw_only=True,\n",
        "                        transform=transform, \n",
        "                        material=train_material)\n",
        "    test_material = [test_mat]\n",
        "    testset = FlakeDataset(data, raw_only=True,\n",
        "                        transform=transform, \n",
        "                        material=test_material)\n",
        "\n",
        "    cam_fold = os.path.join(exp_path, '&'.join(train_material))\n",
        "    cam_train = os.path.join(cam_fold, 'train')\n",
        "    cam_test = os.path.join(cam_fold, 'test')\n",
        "    makedirs(cam_train, cam_test)\n",
        "\n",
        "    target_category = 0\n",
        "    for i in range(len(trainset)):\n",
        "        file_name = extract_filename(trainset.path[i])\n",
        "        img = trainset[i][0]\n",
        "        \n",
        "        input_img = img.unsqueeze(0)\n",
        "        img = img.permute((1, 2, 0))\n",
        "\n",
        "        net = models.resnet18(pretrained=False)\n",
        "        fc_features = net.fc.in_features\n",
        "        net.fc = nn.Linear(fc_features, 1)\n",
        "        net = net.to(device)\n",
        "        net.load_state_dict(\n",
        "            torch.load(torch_dict_path(exp_path, train_material), map_location=device))\n",
        "\n",
        "        grad_cam = GradCam(model=net, feature_module=net.layer4, \\\n",
        "                        target_layer_names=[\"1\"], \n",
        "                        use_cuda=(device==torch.device('cuda')))\n",
        "    \n",
        "        grayscale_cam = grad_cam(input_img, target_category)\n",
        "        grayscale_cam = cv2.resize(grayscale_cam, (img.shape[1], img.shape[0]))\n",
        "        cam = show_cam_on_image(img, grayscale_cam)\n",
        "\n",
        "        cv2.imwrite(os.path.join(cam_train, 'cam_' + file_name + '.jpg'), cam)\n",
        "\n",
        "\n",
        "    for i in range(len(testset)):\n",
        "        file_name = extract_filename(testset.path[i])\n",
        "        img = testset[i][0]\n",
        "        \n",
        "        input_img = img.unsqueeze(0)\n",
        "        img = img.permute((1, 2, 0))\n",
        "\n",
        "        net = models.resnet18(pretrained=False)\n",
        "        fc_features = net.fc.in_features\n",
        "        net.fc = nn.Linear(fc_features, 1)\n",
        "        net = net.to(device)\n",
        "        net.load_state_dict(\n",
        "            torch.load(torch_dict_path(exp_path, train_material), map_location=device))\n",
        "\n",
        "        grad_cam = GradCam(model=net, feature_module=net.layer4, \\\n",
        "                        target_layer_names=[\"1\"], \n",
        "                        use_cuda=(device==torch.device('cuda')))\n",
        "    \n",
        "        grayscale_cam = grad_cam(input_img, target_category)\n",
        "        grayscale_cam = cv2.resize(grayscale_cam, (img.shape[1], img.shape[0]))\n",
        "        cam = show_cam_on_image(img, grayscale_cam)\n",
        "\n",
        "        cv2.imwrite(os.path.join(cam_test, 'cam_' + file_name + '.jpg'), cam)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AToPfRZrcF2K"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpG3dlCSeKJ0"
      },
      "source": [
        "# With quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EJOzKMhTGqP"
      },
      "source": [
        "exp_path = os.path.join(root_path, 'quant')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2KR92QjQpAM"
      },
      "source": [
        "bsz = 4\n",
        "data = pd.read_pickle(os.path.join('/content/drive/Shared drives/2d/data', 'quantized_pad_augment_data.pkl'))\n",
        "data['paths'] = data['paths'].apply(\n",
        "    lambda x: '/content/drive/Shared drives/2d/data/quantized_pad_augment_data/' + x.split('/')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnOBrUTUMmO6"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "materials = ['MoSe2_on_Si', 'MoSe2_on_si_PDMS', 'MoSe2_on_PDMS']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajz_3g98Spra",
        "outputId": "c83a7cd9-a707-4cee-c940-4f87f0d2dad4"
      },
      "source": [
        "for test_mat in materials:\n",
        "    train_material = [mat for mat in materials if mat != test_mat]\n",
        "    trainset = FlakeDataset(data, raw_only=True,\n",
        "                        transform=transform, \n",
        "                        material=train_material)\n",
        "    test_material = [test_mat]\n",
        "    testset = FlakeDataset(data, raw_only=True,\n",
        "                        transform=transform, \n",
        "                        material=test_material)\n",
        "\n",
        "    cam_fold = os.path.join(exp_path, '&'.join(train_material))\n",
        "    cam_train = os.path.join(cam_fold, 'train')\n",
        "    cam_test = os.path.join(cam_fold, 'test')\n",
        "    makedirs(cam_train, cam_test)\n",
        "\n",
        "    target_category = 0\n",
        "    for i in range(len(trainset)):\n",
        "        file_name = extract_filename(trainset.path[i])\n",
        "        img = trainset[i][0]\n",
        "        \n",
        "        input_img = img.unsqueeze(0)\n",
        "        img = img.permute((1, 2, 0))\n",
        "\n",
        "        net = models.resnet18(pretrained=False)\n",
        "        fc_features = net.fc.in_features\n",
        "        net.fc = nn.Linear(fc_features, 1)\n",
        "        net = net.to(device)\n",
        "        net.load_state_dict(\n",
        "            torch.load(torch_dict_path(exp_path, train_material), map_location=device))\n",
        "\n",
        "        grad_cam = GradCam(model=net, feature_module=net.layer4, \\\n",
        "                        target_layer_names=[\"1\"], \n",
        "                        use_cuda=(device==torch.device('cuda')))\n",
        "    \n",
        "        grayscale_cam = grad_cam(input_img, target_category)\n",
        "        grayscale_cam = cv2.resize(grayscale_cam, (img.shape[1], img.shape[0]))\n",
        "        cam = show_cam_on_image(img, grayscale_cam)\n",
        "\n",
        "        cv2.imwrite(os.path.join(cam_train, 'cam_' + file_name + '.jpg'), cam)\n",
        "\n",
        "\n",
        "    for i in range(len(testset)):\n",
        "        file_name = extract_filename(testset.path[i])\n",
        "        img = testset[i][0]\n",
        "        \n",
        "        input_img = img.unsqueeze(0)\n",
        "        img = img.permute((1, 2, 0))\n",
        "\n",
        "        net = models.resnet18(pretrained=False)\n",
        "        fc_features = net.fc.in_features\n",
        "        net.fc = nn.Linear(fc_features, 1)\n",
        "        net = net.to(device)\n",
        "        net.load_state_dict(\n",
        "            torch.load(torch_dict_path(exp_path, train_material), map_location=device))\n",
        "\n",
        "        grad_cam = GradCam(model=net, feature_module=net.layer4, \\\n",
        "                        target_layer_names=[\"1\"], \n",
        "                        use_cuda=(device==torch.device('cuda')))\n",
        "    \n",
        "        grayscale_cam = grad_cam(input_img, target_category)\n",
        "        grayscale_cam = cv2.resize(grayscale_cam, (img.shape[1], img.shape[0]))\n",
        "        cam = show_cam_on_image(img, grayscale_cam)\n",
        "\n",
        "        cv2.imwrite(os.path.join(cam_test, 'cam_' + file_name + '.jpg'), cam)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcoiTYGeQLHn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47kbPskfKj44"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_2bs_WlKj7M"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTBFQYKUSBhI"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHiOYq91SLrh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-GavZ0cSoVK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}